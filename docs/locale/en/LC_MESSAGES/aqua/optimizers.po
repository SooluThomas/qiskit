# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2019, Qiskit Development Team
# This file is distributed under the same license as the Qiskit package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qiskit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-09-26 15:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../aqua/optimizers.rst:5
msgid "Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:7
msgid ""
"Aqua  contains a variety of classical optimizers for use by quantum "
"variational algorithms, such as :ref:`vqe`. Logically, these optimizers "
"can be divided into two categories:"
msgstr ""

#: ../../aqua/optimizers.rst:11
msgid ""
":ref:`local-optimizers`: Given an optimization problem, a *local "
"optimizer* is a function that attempts to find an optimal value within "
"the neighboring set of a candidate solution."
msgstr ""

#: ../../aqua/optimizers.rst:14
msgid ""
":ref:`global-optimizers`: Given an optimization problem, a *global "
"optimizer* is a function that attempts to find an optimal value among all"
" possible solutions."
msgstr ""

msgid "Extending the Optimizer Library"
msgstr ""

#: ../../aqua/optimizers.rst:20
msgid ""
"Consistent with its unique  design, Aqua has a modular and extensible "
"architecture. Algorithms and their supporting objects, such as optimizers"
" for quantum variational algorithms,  are pluggable modules in Aqua. New "
"optimizers for quantum variational algorithms are typically installed in "
"the ``qiskit/aqua/components/optimizers`` folder and derive from the "
"``Optimizer`` class. Aqua also allows for :ref:`aqua-dynamically-"
"discovered-components`: new optimizers can register themselves as Aqua "
"extensions and be dynamically discovered at run time independent of their"
" location in the file system. This is done in order to encourage "
"researchers and developers interested in :ref:`aqua-extending` to extend "
"the Aqua framework with their novel research contributions."
msgstr ""

#: ../../aqua/optimizers.rst:35
msgid ""
"Section :ref:`aqua-extending` provides more details on how to extend Aqua"
" with new components."
msgstr ""

#: ../../aqua/optimizers.rst:42
msgid "Local Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:44
msgid ""
"This section presents the classical local optimizers made available in "
"Aqua. These optimizers are meant to be used in conjunction with quantum "
"variational algorithms:"
msgstr ""

#: ../../aqua/optimizers.rst:48
msgid ":ref:`adam_amsgrad`"
msgstr ""

#: ../../aqua/optimizers.rst:49
msgid ":ref:`aqgd`"
msgstr ""

#: ../../aqua/optimizers.rst:50
msgid ":ref:`cg`"
msgstr ""

#: ../../aqua/optimizers.rst:51
msgid ":ref:`cobyla`"
msgstr ""

#: ../../aqua/optimizers.rst:52
msgid ":ref:`l-bfgs-b`"
msgstr ""

#: ../../aqua/optimizers.rst:53
msgid ":ref:`nelder-mead`"
msgstr ""

#: ../../aqua/optimizers.rst:54
msgid ":ref:`p-bfgs`"
msgstr ""

#: ../../aqua/optimizers.rst:55
msgid ":ref:`powell`"
msgstr ""

#: ../../aqua/optimizers.rst:56
msgid ":ref:`slsqp`"
msgstr ""

#: ../../aqua/optimizers.rst:57
msgid ":ref:`spsa`"
msgstr ""

#: ../../aqua/optimizers.rst:58
msgid ":ref:`tnc`"
msgstr ""

#: ../../aqua/optimizers.rst:60
msgid ""
"Except for :ref:`adam_amsgrad`, :ref:`aqgd` and :ref:`p-bfgs`, all these "
"optimizers are directly based on the ``scipy.optimize.minimize`` "
"optimization function in the `SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__"
" Python library. They all have a common pattern for parameters. "
"Specifically, the ``tol`` parameter, whose value must be a ``float`` "
"indicating *tolerance for termination*, is from the "
"``scipy.optimize.minimize``  method itself, while the remaining "
"parameters are from the `options dictionary "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html>`__,"
" which may be referred to for further information."
msgstr ""

msgid "Transparent Parallelization of Gradient-based Local Opitmizers"
msgstr ""

#: ../../aqua/optimizers.rst:72
msgid ""
"Aqua comes with a large collection of adaptive algorithms, such as the "
"`Variational Quantum Eigensolver (VQE) algorithm "
"<https://www.nature.com/articles/ncomms5213>`__, `Quantum Approximate "
"Optimization Algorithm (QAOA) <https://arxiv.org/abs/1411.4028>`__, the "
"`Quantum Support Vector Machine (SVM) Variational Algorithm "
"<https://arxiv.org/abs/1804.11326>`__ for AI. All these algorithms "
"interleave quantum and classical computations, making use of classical "
"optimizers. Aqua includes nine local and five global optimizers to choose"
" from. By profiling the execution of the adaptive algorithms, we have "
"detected that a large portion of the execution time is taken by the "
"optimization phase, which runs classically. Among the most widely used "
"optimizers are the *gradient-based* ones; these optimizers attempt to "
"compute the absolute minimum (or maximum) of a function :math:`f` through"
" its gradient."
msgstr ""

#: ../../aqua/optimizers.rst:87
msgid ""
"Seven local optimizers among those integrated into Aqua are gradient-"
"based: the four local optimizers *Limited-memory Broyden-Fletcher-"
"Goldfarb-Shanno Bound (L-BFGS-B)*, *Sequential Least SQuares Programming "
"(SLSQP)*, *Conjugate Gradient (CG)*, and *Truncated Newton (TNC)* from "
"`SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__,"
" as well as `Simultaneous Perturbation Stochastic Approximation (SPSA) "
"<https://www.jhuapl.edu/SPSA/>`__, *ADAM* and *Analytic Quantum Gradient "
"Descent (AQGD)*. Aqua contains a methodology that parallelizes the "
"classical computation of the partial derivatives in the gradient-based "
"local optimizers listed above. This parallelization takes place "
"*transparently*, in the sense that Aqua intercepts the computation of the"
" partial derivatives and parallelizes it without making any change to the"
" actual source code of the optimizers."
msgstr ""

#: ../../aqua/optimizers.rst:101
#, python-format
msgid ""
"In order to activate the parallelization mechanism for an adaptive "
"algorithm included in Aqua, it is sufficient to construct it with "
"parameter ``batch_mode`` set to ``True``. Our experiments have proven "
"empirically that parallelizing the process of a gradient-based local "
"optimizer achieves a 30% speedup in the execution time of an adaptive "
"algorithms on a simulator."
msgstr ""

#: ../../aqua/optimizers.rst:112
msgid "ADAM"
msgstr ""

#: ../../aqua/optimizers.rst:113
msgid ""
"ADAM is a gradient-based optimization algorithm that is relies on "
"adaptive estimates of lower-order moments. The algorithm requires little "
"memory and is invariant to diagonal rescaling of the gradients. "
"Furthermore, it is able to cope with non-stationary objective functions "
"and noisy and/or sparse gradients. AMSGRAD (a variant of ADAM) uses a "
"'long-term memory' of past gradients and, thereby, improves convergence "
"properties."
msgstr ""

#: ../../aqua/optimizers.rst:119
msgid ""
"Kingma, Diederik & Ba, Jimmy. (2014). Adam: A Method for Stochastic "
"Optimization. International Conference on Learning Representations."
msgstr ""

#: ../../aqua/optimizers.rst:122
msgid ""
"Sashank J. Reddi and Satyen Kale and Sanjiv Kumar. (2018). On the "
"Convergence of Adam and Beyond. International Conference on Learning "
"Representations."
msgstr ""

#: ../../aqua/optimizers.rst:125 ../../aqua/optimizers.rst:224
#: ../../aqua/optimizers.rst:284 ../../aqua/optimizers.rst:400
#: ../../aqua/optimizers.rst:468 ../../aqua/optimizers.rst:579
#: ../../aqua/optimizers.rst:644 ../../aqua/optimizers.rst:810
msgid "The following parameters are supported:"
msgstr ""

#: ../../aqua/optimizers.rst:127 ../../aqua/optimizers.rst:226
msgid "The maximum number of iterations to perform."
msgstr ""

#: ../../aqua/optimizers.rst:133 ../../aqua/optimizers.rst:292
msgid "This parameters takes a positive ``int`` value.  The default is ``20``."
msgstr ""

#: ../../aqua/optimizers.rst:135 ../../aqua/optimizers.rst:242
msgid "The tolerance for termination."
msgstr ""

#: ../../aqua/optimizers.rst:141 ../../aqua/optimizers.rst:248
msgid "The default value is ``1e-06``."
msgstr ""

#: ../../aqua/optimizers.rst:143 ../../aqua/optimizers.rst:234
msgid "The learning rate:"
msgstr ""

#: ../../aqua/optimizers.rst:149
msgid "The default value is ``1e-03``."
msgstr ""

#: ../../aqua/optimizers.rst:151
msgid ""
"First hyper-parameter used for the evaluation of the first moment "
"estimate."
msgstr ""

#: ../../aqua/optimizers.rst:157
msgid "The default value is ``0.9``."
msgstr ""

#: ../../aqua/optimizers.rst:159
msgid ""
"Second hyper-parameter used for the evaluation of the second moment "
"estimate."
msgstr ""

#: ../../aqua/optimizers.rst:165
msgid "The default value is ``0.99``."
msgstr ""

#: ../../aqua/optimizers.rst:167
msgid "Noise factor used for reasons of numerical stability."
msgstr ""

#: ../../aqua/optimizers.rst:173
msgid "The default value is ``1e-8``."
msgstr ""

#: ../../aqua/optimizers.rst:175 ../../aqua/optimizers.rst:321
#: ../../aqua/optimizers.rst:680
msgid "Step size used for numerical approximation of the Jacobian."
msgstr ""

#: ../../aqua/optimizers.rst:181
msgid "The default value is ``1e-10``."
msgstr ""

#: ../../aqua/optimizers.rst:183
msgid "A Boolean value indicating whether or not to use the AMSGRAD variant."
msgstr ""

#: ../../aqua/optimizers.rst:189 ../../aqua/optimizers.rst:256
#: ../../aqua/optimizers.rst:300 ../../aqua/optimizers.rst:358
#: ../../aqua/optimizers.rst:826
msgid "The default value is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:192
msgid ""
"A string indicating a directory for storing optimizer's parameters. If "
"``None`` then the parameters will not be stored."
msgstr ""

#: ../../aqua/optimizers.rst:199
msgid "The default value is ``None``."
msgstr ""

msgid "Declarative Name"
msgstr ""

#: ../../aqua/optimizers.rst:203
msgid ""
"When referring to ADAM declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ADAM``."
msgstr ""

#: ../../aqua/optimizers.rst:211
msgid "Analytic Quantum Gradient Descent (AQGD)"
msgstr ""

#: ../../aqua/optimizers.rst:212
msgid ""
"Analytic Quantum Gradient Descent (AQGD) performs gradient descent "
"optimization with a momentum term and analytic gradients for parametrized"
" quantum gates, i.e. Pauli Rotations. See e.g.:"
msgstr ""

#: ../../aqua/optimizers.rst:216
msgid ""
"K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. (2018). Quantum "
"circuit learning.Phys. Rev. A 98, 032309."
msgstr ""

#: ../../aqua/optimizers.rst:219
msgid ""
"Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, Nathan "
"Killoran. (2019). Evaluating analytic gradients on quantum hardware. "
"Phys. Rev. A 99, 032331."
msgstr ""

#: ../../aqua/optimizers.rst:222
msgid "for further details on analytic gradients of parametrized quantum gates."
msgstr ""

#: ../../aqua/optimizers.rst:232
msgid "This parameters takes a positive ``int`` value.  The default is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:240
msgid "The default value is ``3.0``."
msgstr ""

#: ../../aqua/optimizers.rst:250
msgid "A Boolean value indicating whether or not to display convergence messages."
msgstr ""

#: ../../aqua/optimizers.rst:258
msgid ""
"Bias towards the previous gradient momentum. Must be within the bounds: "
"[0,1) .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:261
msgid "momentum : float"
msgstr ""

#: ../../aqua/optimizers.rst:263
msgid "The default value is ``0.25``."
msgstr ""

#: ../../aqua/optimizers.rst:267
msgid ""
"When referring to AQGD declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``AQGD``."
msgstr ""

#: ../../aqua/optimizers.rst:277
msgid "Conjugate Gradient (CG) Method"
msgstr ""

#: ../../aqua/optimizers.rst:278
msgid ""
"CG is an algorithm for the numerical solution of systems of linear "
"equations whose matrices are symmetric and positive-definite. It is an "
"*iterative algorithm* in that it uses an initial guess to generate a "
"sequence of improving approximate solutions for a problem, in which each "
"approximation is derived from the previous ones.  It is often used to "
"solve unconstrained optimization problems, such as energy minimization."
msgstr ""

#: ../../aqua/optimizers.rst:286 ../../aqua/optimizers.rst:344
msgid "The maximum number of iterations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:294 ../../aqua/optimizers.rst:352
#: ../../aqua/optimizers.rst:820
msgid "A Boolean value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:302
msgid ""
"A tolerance value that must be greater than the gradient norm before "
"successful termination."
msgstr ""

#: ../../aqua/optimizers.rst:309
msgid "The default value is ``1e-05``."
msgstr ""

#: ../../aqua/optimizers.rst:312 ../../aqua/optimizers.rst:368
#: ../../aqua/optimizers.rst:504 ../../aqua/optimizers.rst:616
#: ../../aqua/optimizers.rst:671 ../../aqua/optimizers.rst:864
msgid "The tolerance for termination:"
msgstr ""

#: ../../aqua/optimizers.rst:318
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float`` value, otherwise, it is set to ``None``.  The default"
" is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:327 ../../aqua/optimizers.rst:878
msgid "The default value is ``1.4901161193847656e-08``."
msgstr ""

#: ../../aqua/optimizers.rst:331
msgid ""
"When referring to CG declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``CG``."
msgstr ""

#: ../../aqua/optimizers.rst:338
msgid "Constrained Optimization BY Linear Approximation (COBYLA)"
msgstr ""

#: ../../aqua/optimizers.rst:340
msgid ""
"COBYLA is a numerical optimization method for constrained problems where "
"the derivative of the objective function is not known. COBYLA supports "
"the following parameters:"
msgstr ""

#: ../../aqua/optimizers.rst:350 ../../aqua/optimizers.rst:408
#: ../../aqua/optimizers.rst:485
msgid "A positive ``int`` value is expected.  The default is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:360
msgid "Reasonable initial changes to the variable:"
msgstr ""

#: ../../aqua/optimizers.rst:366
msgid "The default value is ``1.0``."
msgstr ""

#: ../../aqua/optimizers.rst:374
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is set to ``None``. The default "
"is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:379
msgid ""
"When referring to COBYLA declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``COBYLA``."
msgstr ""

#: ../../aqua/optimizers.rst:386
msgid "Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)"
msgstr ""

#: ../../aqua/optimizers.rst:388
msgid ""
"The target goal of L-BFGS-B is to minimize the value of a differentiable "
"scalar function :math:`f`. This optimizer is a *quasi-Newton method*, "
"meaning that, in contrast to *Newtons's method*, it does not require "
":math:`f`'s *Hessian* (the matrix of :math:`f`'s second derivatives) when"
" attempting to compute :math:`f`'s minimum value. Like BFGS, L-BFGS is an"
" iterative method for solving unconstrained, non-linear optimization "
"problems, but approximates BFGS using a limited amount of computer "
"memory. L-BFGS starts with an initial estimate of the optimal value, and "
"proceeds iteratively to refine that estimate with a sequence of better "
"estimates. The derivatives of :math:`f` are used to identify the "
"direction of steepest descent, and also to form an estimate of the "
"Hessian matrix (second derivative) of :math:`f`. L-BFGS-B extends L-BFGS "
"to handle simple, per-variable bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:402
msgid "The maximum number of function evaluations:"
msgstr ""

#: ../../aqua/optimizers.rst:410 ../../aqua/optimizers.rst:470
#: ../../aqua/optimizers.rst:581 ../../aqua/optimizers.rst:646
#: ../../aqua/optimizers.rst:812
msgid "The maximum number of iterations:"
msgstr ""

#: ../../aqua/optimizers.rst:416
msgid "A positive ``int`` value is expected.  The default is ``15000``."
msgstr ""

#: ../../aqua/optimizers.rst:418
msgid "Accuracy factor:"
msgstr ""

#: ../../aqua/optimizers.rst:424
msgid "A positive ``int`` value is expected.  The default is ``10``."
msgstr ""

#: ../../aqua/optimizers.rst:426
msgid ""
"An ``int`` value controlling the frequency of the printed output showing "
"the optimizer's operations:"
msgstr ""

#: ../../aqua/optimizers.rst:433
msgid "The default is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:435
msgid "Step size used if numerically calculating the gradient."
msgstr ""

#: ../../aqua/optimizers.rst:441 ../../aqua/optimizers.rst:686
msgid "The default value is ``1e-08``."
msgstr ""

#: ../../aqua/optimizers.rst:444
msgid ""
"Further detailed information on ``factr`` and ``iprint`` may be found at "
"`scipy.optimize.fmin_l_bfgs_b "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`__."
msgstr ""

#: ../../aqua/optimizers.rst:449
msgid ""
"When referring to L-BFGS-B declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``L_BFGS_B``."
msgstr ""

#: ../../aqua/optimizers.rst:456
msgid "Nelder-Mead"
msgstr ""

#: ../../aqua/optimizers.rst:458
msgid ""
"The Nelder-Mead algorithm performs unnconstrained optimization; it "
"ignores bounds or constraints.  It is used to find the minimum or maximum"
" of an objective function in a multidimensional space.  It is based on "
"the Simplex algorithm. Nelder-Mead is robust in many applications, "
"especially when the first and second derivatives of the objective "
"function are not known. However, if the numerical computation of the "
"derivatives can be trusted to be accurate, other algorithms using the "
"first and/or second derivatives information might be preferred to Nelder-"
"Mead for their better performance in the general case, especially in "
"consideration of the fact that the Nelder–Mead technique is a heuristic "
"search method that can converge to non-stationary points."
msgstr ""

#: ../../aqua/optimizers.rst:476
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a positive ``int``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:479 ../../aqua/optimizers.rst:591
msgid "The maximum number of functional evaluations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:487 ../../aqua/optimizers.rst:599
#: ../../aqua/optimizers.rst:654
msgid "A ``bool`` value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:493 ../../aqua/optimizers.rst:517
#: ../../aqua/optimizers.rst:605 ../../aqua/optimizers.rst:660
msgid "The default is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:495 ../../aqua/optimizers.rst:607
msgid ""
"A tolerance parameter indicating the absolute error in ``xopt`` between "
"iterations that will be considered acceptable for convergence."
msgstr ""

#: ../../aqua/optimizers.rst:502 ../../aqua/optimizers.rst:614
msgid "The default value is ``0.0001``."
msgstr ""

#: ../../aqua/optimizers.rst:510 ../../aqua/optimizers.rst:622
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:519
msgid "If true will adapt algorithm to dimensionality of problem."
msgstr ""

#: ../../aqua/optimizers.rst:523
msgid ""
"When referring to Nelder-Mead declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``NELDER_MEAD``."
msgstr ""

#: ../../aqua/optimizers.rst:530
msgid "Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)"
msgstr ""

#: ../../aqua/optimizers.rst:532
msgid ""
"P-BFGS is a parallellized version of `L-BFGS-B <#limited-memory-broyden-"
"fletcher-goldfarb-shanno-bound-l-bfgs-b>`__, with which it shares the "
"same parameters. P-BFGS can be useful when the target hardware is a "
"quantum simulator running on a classical machine. This allows the "
"multiple processes to use simulation to potentially reach a minimum "
"faster. The parallelization may help the optimizer avoid getting stuck at"
" local optima.  In addition to the parameters of L-BFGS-B, P-BFGS "
"supports an following parameter --- the maximum number of processes "
"spawned by P-BFGS:"
msgstr ""

#: ../../aqua/optimizers.rst:546
msgid ""
"By default, P-BFGS runs one optimization in the current process and "
"spawns additional processes up to the number of processor cores. An "
"``int`` value may be specified to limit the total number of processes (or"
" cores) used.  This parameter is optional.  If specified, the value of "
"this parameter must be a positive ``int``, otherwise, it is ``None``.  "
"The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:554
msgid ""
"The parallel processes do not currently work for this optimizer on the "
"Microsoft Windows platform. There, P-BFGS will just run the one "
"optimization in the main process, without spawning new processes. "
"Therefore, the resulting behavior will be the same as the L-BFGS-B "
"optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:562
msgid ""
"When referring to P-BFGS declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``P_BFGS``."
msgstr ""

#: ../../aqua/optimizers.rst:570
msgid "Powell"
msgstr ""

#: ../../aqua/optimizers.rst:572
msgid ""
"The Powell algorithm performs unconstrained optimization; it ignores "
"bounds or constraints. Powell is a *conjugate direction method*: it "
"performs sequential one-dimensional minimization along each directional "
"vector, which is updated at each iteration of the main minimization loop."
" The function being minimized need not be differentiable, and no "
"derivatives are taken."
msgstr ""

#: ../../aqua/optimizers.rst:587
msgid ""
"This parameter is optional. If specified, the value of this parameter "
"must be a positive ``int``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:597 ../../aqua/optimizers.rst:728
msgid "A positive ``int`` value is expected.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:627
msgid ""
"When referring to Powell declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``POWELL``."
msgstr ""

#: ../../aqua/optimizers.rst:634
msgid "Sequential Least SQuares Programming (SLSQP)"
msgstr ""

#: ../../aqua/optimizers.rst:636
msgid ""
"SLSQP minimizes a function of several variables with any combination of "
"bounds, equality and inequality constraints. The method wraps the SLSQP "
"Optimization subroutine originally implemented by Dieter Kraft. SLSQP is "
"ideal for  mathematical problems for which the objective function and the"
" constraints are twice continuously differentiable. Note that the wrapper"
" handles infinite values in bounds by converting them into large floating"
" values."
msgstr ""

#: ../../aqua/optimizers.rst:652 ../../aqua/optimizers.rst:818
msgid "A positive ``int`` value is expected.  The default is ``100``."
msgstr ""

#: ../../aqua/optimizers.rst:662
msgid ""
"A tolerance value indicating precision goal for the value of the "
"objective function in the stopping criterion."
msgstr ""

#: ../../aqua/optimizers.rst:669
msgid "A ``float`` value is expected.  The default value is ``1e-06``."
msgstr ""

#: ../../aqua/optimizers.rst:677
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:690
msgid ""
"When referring to SLSQP declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SLSQP``."
msgstr ""

#: ../../aqua/optimizers.rst:697
msgid "Simultaneous Perturbation Stochastic Approximation (SPSA)"
msgstr ""

#: ../../aqua/optimizers.rst:699
msgid ""
"SPSA is an algorithmic method for optimizing systems with multiple "
"unknown parameters. As an optimization method, it is appropriately suited"
" to large-scale population models, adaptive modeling, and simulation "
"optimization."
msgstr ""

#: ../../aqua/optimizers.rst:704
msgid ""
"Many examples are presented at the `SPSA Web site "
"<http://www.jhuapl.edu/SPSA>`__."
msgstr ""

#: ../../aqua/optimizers.rst:706
msgid ""
"SPSA is a descent method capable of finding global minima, sharing this "
"property with other methods as simulated annealing. Its main feature is "
"the gradient approximation, which requires only two measurements of the "
"objective function, regardless of the dimension of the optimization "
"problem."
msgstr ""

#: ../../aqua/optimizers.rst:713
msgid ""
"SPSA can be used in the presence of noise, and it is therefore indicated "
"in situations involving measurement uncertainty on a quantum computation "
"when finding a minimum. If you are executing a variational algorithm "
"using a Quantum ASseMbly Language (QASM) simulator or a real device, SPSA"
" would be the most recommended choice among the optimizers provided here."
msgstr ""

#: ../../aqua/optimizers.rst:718
msgid ""
"The optimization process includes a calibration phase, which requires "
"additional functional evaluations.  Overall, the following parameters are"
" supported:"
msgstr ""

#: ../../aqua/optimizers.rst:721
msgid ""
"Maximum number of trial steps to be taken for the optimization. There are"
" two function evaluations per trial:"
msgstr ""

#: ../../aqua/optimizers.rst:730
msgid ""
"An ``int`` value determining how often optimization outcomes should be "
"stored during execution:"
msgstr ""

#: ../../aqua/optimizers.rst:736
msgid ""
"A positive ``int`` value is expected. SPSA will store optimization "
"outcomes every ``save_steps`` trial steps. The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:740
msgid ""
"The number of last updates of the variables to average on for the final "
"objective function:"
msgstr ""

#: ../../aqua/optimizers.rst:747
msgid "A positive ``int`` value is expected.  The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:749
msgid "Control parameters for SPSA:"
msgstr ""

#: ../../aqua/optimizers.rst:759
msgid ""
"These are the SPSA control parameters, consisting of 5 ``float`` values, "
"and are used as described below."
msgstr ""

#: ../../aqua/optimizers.rst:762
msgid ""
"SPSA updates the parameters (``theta``) for the objective function "
"(``J``) through the following equation at iteration ``k``:"
msgstr ""

#: ../../aqua/optimizers.rst:774
msgid ""
"``J(theta)`` is the  objective value of ``theta``. ``c0``, ``c1``, "
"``c2``, ``c3`` and ``c4`` are the five control parameters. By default, "
"``c0`` is calibrated through a few evaluations on the objective function "
"with the initial ``theta``. ``c1``, ``c2``, ``c3`` and ``c4`` are set as "
"``0.1``, ``0.602``, ``0.101``, ``0.0``, respectively."
msgstr ""

#: ../../aqua/optimizers.rst:781
msgid "Calibration step for SPSA."
msgstr ""

#: ../../aqua/optimizers.rst:787
msgid ""
"The default value is ``False``. When calibration is done, i.e. when "
"``skip_calibration`` is ``False`` (by default) the control parameter "
"``c0`` as supplied is adjusted by the calibration step before "
"optimization. If ``skip_calibration`` is ``True`` then the calibration "
"step, which occurs ahead of optimization, is skipped and ``c0`` will be "
"used unaltered."
msgstr ""

#: ../../aqua/optimizers.rst:796
msgid ""
"When referring to SPSA declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SPSA``."
msgstr ""

#: ../../aqua/optimizers.rst:803
msgid "Truncated Newton (TNC)"
msgstr ""

#: ../../aqua/optimizers.rst:804
msgid ""
"TNC uses a truncated Newton algorithm to minimize a function with "
"variables subject to bounds. This algorithm uses gradient information; it"
" is also called Newton Conjugate-Gradient. It differs from the :ref:`cg` "
"method as it wraps a C implementation and allows each variable to be "
"given upper and lower bounds."
msgstr ""

#: ../../aqua/optimizers.rst:828
msgid "Relative precision for finite difference calculations:"
msgstr ""

#: ../../aqua/optimizers.rst:834
msgid "The default value is ``0.0``."
msgstr ""

#: ../../aqua/optimizers.rst:836
msgid ""
"A tolerance value indicating the precision goal for the value of the "
"objective function ``f`` in the stopping criterion."
msgstr ""

#: ../../aqua/optimizers.rst:843 ../../aqua/optimizers.rst:852
#: ../../aqua/optimizers.rst:862
msgid "The default value is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:845
msgid ""
"A tolerance value indicating precision goal for the value of ``x`` in the"
" stopping criterion, after applying ``x`` scaling factors."
msgstr ""

#: ../../aqua/optimizers.rst:854
msgid ""
"A tolerance value indicating precision goal for the value of the "
"projected gradient ``g`` in the stopping criterion, after applying ``x`` "
"scaling factors."
msgstr ""

#: ../../aqua/optimizers.rst:870
msgid ""
"This parameter is optional. If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``"
msgstr ""

#: ../../aqua/optimizers.rst:873
msgid ""
"Step size used for numerical approximation of the Jacobian. .. code:: "
"python"
msgstr ""

#: ../../aqua/optimizers.rst:876
msgid "eps : float"
msgstr ""

#: ../../aqua/optimizers.rst:882
msgid ""
"When referring to TNC declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``TNC``."
msgstr ""

#: ../../aqua/optimizers.rst:889
msgid "Global Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:890
msgid ""
"Aqua supports a number of classical global optimizers, all based on the "
"open-source `NonLinear optimization (NLopt) library "
"<https://nlopt.readthedocs.io>`__. Each of these optimizers uses the "
"corresponding named optimizer from NLopt. This package has native code "
"implementations and must be installed locally for these global optimizers"
" to be accessible by Aqua. Wrapper code allowing Aqua to interface these "
"optimizers is installed in the ``nlopt`` subfolder of the ``optimizers`` "
"folder."
msgstr ""

msgid "Installation of NLopt"
msgstr ""

#: ../../aqua/optimizers.rst:900
msgid ""
"The `NLopt download and installation instructions "
"<https://nlopt.readthedocs.io/en/latest/#download-and-installation>`__ "
"describe how to install NLopt."
msgstr ""

#: ../../aqua/optimizers.rst:903
msgid ""
"If you running Aqua on Windows, then you might want to refer to the "
"specific `instructions for NLopt on Windows "
"<https://nlopt.readthedocs.io/en/latest/NLopt_on_Windows/>`__."
msgstr ""

#: ../../aqua/optimizers.rst:906
msgid ""
"If you are running Aqua on a Unix-like system, first ensure that your "
"environment is set to the Python executable for which the qiskit_aqua "
"package is installed and running. Now, having downloaded and unpacked the"
" NLopt archive file (for example, ``nlopt-2.4.2.tar.gz`` for version "
"2.4.2), enter the following commands:"
msgstr ""

#: ../../aqua/optimizers.rst:917
msgid ""
"The above makes and installs the shared libraries and Python interface in"
" `/usr/local`. To have these be used by Aqua, the following commands can "
"be entered to augment the dynamic library load path and python path "
"respectively, assuming that you choose to leave these entities where they"
" were built and installed as per above commands and that you are running "
"Python 3.6:"
msgstr ""

#: ../../aqua/optimizers.rst:928
msgid ""
"The two ``export`` commands above can be pasted into the "
"``.bash_profile`` file in the user's home directory for automatic "
"execution.  Now you can run Aqua and these optimizers should be available"
" for you to use."
msgstr ""

msgid "The ``max_evals`` Parameter"
msgstr ""

#: ../../aqua/optimizers.rst:934
msgid ""
"All the NLopt optimizers are supported by a common interface, allowing "
"the optimizers to share the same common parameters. For quantum "
"variational algorithms, it is necessary to assign a value to the "
"following parameter:"
msgstr ""

#: ../../aqua/optimizers.rst:943
msgid ""
"This parameter takes a positive ``int`` as its value, indicating the "
"maximum object function evaluation.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:946
msgid "Currently, Aqua supplies the following global optimizers from NLOpt:"
msgstr ""

#: ../../aqua/optimizers.rst:948
msgid ":ref:`crs`"
msgstr ""

#: ../../aqua/optimizers.rst:949
msgid ":ref:`direct-l`"
msgstr ""

#: ../../aqua/optimizers.rst:950
msgid ":ref:`direct-l-rand)`"
msgstr ""

#: ../../aqua/optimizers.rst:951
msgid ":ref:`esch`"
msgstr ""

#: ../../aqua/optimizers.rst:952
msgid ":ref:`isres`"
msgstr ""

#: ../../aqua/optimizers.rst:958
msgid "Controller Random Search (CRS) with Local Mutation"
msgstr ""

#: ../../aqua/optimizers.rst:959
msgid ""
"`CRS with local mutation "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms\\ /#controlled-"
"random-search-crs-with-local-mutation>`__ is part of the family of the "
"CRS optimizers. The CRS optimizers start with a random population of "
"points, and randomly evolve these points by heuristic rules. In the case "
"of CRS with local mutation, the evolution is a randomized version of the "
":ref:`nelder-mead` local optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:969
msgid ""
"When referring to CRS with local mutation declaratively inside Aqua, its "
"code ``name``, by which Aqua dynamically discovers and loads it, is "
"``CRS``."
msgstr ""

#: ../../aqua/optimizers.rst:976
msgid "DIviding RECTangles algorithm - Locally based (DIRECT-L)"
msgstr ""

#: ../../aqua/optimizers.rst:978
msgid ""
"DIviding RECTangles (DIRECT) is a deterministic-search algorithms based "
"on systematic division of the search domain into increasingly smaller "
"hyperrectangles. The `DIRECT-L "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-"
"direct-l>`__ version is a variant of DIRECT that makes the algorithm more"
" biased towards local search, so that it is more efficient for functions "
"with few local minima."
msgstr ""

#: ../../aqua/optimizers.rst:986
msgid ""
"When referring to DIRECT-L declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``DIRECT_L``."
msgstr ""

#: ../../aqua/optimizers.rst:993
msgid "DIviding RECTangles algorithm - Locally based - RANDomized (DIRECT-L-RAND)"
msgstr ""

#: ../../aqua/optimizers.rst:995
msgid ""
"`DIRECT-L-RAND <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms"
"/#direct-and-direct-l>`__ is a variant of :ref:`direct-l` that uses some "
"randomization to help decide which dimension to halve next in the case of"
" near-ties."
msgstr ""

#: ../../aqua/optimizers.rst:1001
msgid ""
"When referring to DIRECT-L-RAND declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``DIRECT_L_RAND``."
msgstr ""

#: ../../aqua/optimizers.rst:1008
msgid "Evolutionary Strategy algorithm with CaucHy distribution (ESCH)"
msgstr ""

#: ../../aqua/optimizers.rst:1010
msgid ""
"`ESCH <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#esch-"
"evolutionary-algorithm>`__ is an evolutionary algorithm for global "
"optimization that supports bound constraints only. Specifically, it does "
"not support nonlinear constraints."
msgstr ""

#: ../../aqua/optimizers.rst:1016
msgid ""
"When referring to ESCH declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ESCH``."
msgstr ""

#: ../../aqua/optimizers.rst:1023
msgid "Improved Stochastic Ranking Evolution Strategy (ISRES)"
msgstr ""

#: ../../aqua/optimizers.rst:1025
msgid ""
"`ISRES <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms\\ /#isres-"
"improved-stochastic-ranking-evolution-strategy>`__ is an algorithm for "
"nonlinearly-constrained global optimization. It has heuristics to escape "
"local optima, even though convergence to a global optima is not "
"guaranteed. The evolution strategy is based on a combination of a "
"mutation rule and differential variation. The fitness ranking is simply "
"via the objective function for problems without nonlinear constraints. "
"When nonlinear constraints are included, the `stochastic ranking proposed"
" by Runarsson and Yao "
"<https://notendur.hi.is/^tpr/software/sres/Tec311r.pdf>`__ is employed. "
"This method supports arbitrary nonlinear inequality and equality "
"constraints, in addition to the bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:1039
msgid ""
"When referring to ISRES declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ISRES``."
msgstr ""

